{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "460fce7c",
   "metadata": {},
   "source": [
    "# Scenario Selection Evaluation\n",
    "\n",
    "This notebook validates the selected scenarios, plots representative graphs, and computes clustering metrics (e.g., silhouette score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7666a287",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-26T18:26:46.103897Z",
     "iopub.status.busy": "2026-01-26T18:26:46.103609Z",
     "iopub.status.idle": "2026-01-26T18:26:49.160643Z",
     "shell.execute_reply": "2026-01-26T18:26:49.160380Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROOT: /Users/ignaciagothe/Desktop/proyecto graph kernels/graph_scenarios\n",
      "GRAPHS_DIR: /Users/ignaciagothe/Desktop/proyecto graph kernels/graph_scenarios/data/sub20/graphs\n",
      "OUTPUT_DIR: /Users/ignaciagothe/Desktop/proyecto graph kernels/graph_scenarios/src/kernels/outputs\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "import os\n",
    "import glob\n",
    "import pickle\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import silhouette_score, silhouette_samples\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "def find_repo_root(start=None):\n",
    "    if start is None:\n",
    "        start = Path.cwd()\n",
    "    start = start.resolve()\n",
    "    for parent in [start] + list(start.parents):\n",
    "        if (parent / '.git').exists():\n",
    "            return parent\n",
    "        if (parent / 'data' / 'sub20' / 'graphs').exists():\n",
    "            return parent\n",
    "    raise FileNotFoundError(\"Could not find repo root (no .git or data/sub20/graphs found)\")\n",
    "\n",
    "\n",
    "ROOT = find_repo_root()\n",
    "\n",
    "# Prefer repo-root graphs/ if it exists, otherwise fall back to data/sub20/graphs\n",
    "if (ROOT / 'graphs').exists():\n",
    "    GRAPHS_DIR = ROOT / 'graphs'\n",
    "else:\n",
    "    GRAPHS_DIR = ROOT / 'data' / 'sub20' / 'graphs'\n",
    "\n",
    "OUTPUT_DIR = ROOT / 'src' / 'kernels' / 'outputs'\n",
    "OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "print('ROOT:', ROOT)\n",
    "print('GRAPHS_DIR:', GRAPHS_DIR)\n",
    "print('OUTPUT_DIR:', OUTPUT_DIR)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "86230939",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-26T18:26:49.162092Z",
     "iopub.status.busy": "2026-01-26T18:26:49.161931Z",
     "iopub.status.idle": "2026-01-26T18:26:49.170035Z",
     "shell.execute_reply": "2026-01-26T18:26:49.169783Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k=20 | format=selected_graphs | path=selected_graphs_k20.json\n",
      "k=100 | format=selected_graphs | path=selected_graphs_k100.json\n"
     ]
    }
   ],
   "source": [
    "# Load selection outputs\n",
    "K_VALUES = [20, 100]\n",
    "\n",
    "\n",
    "def load_selection(k):\n",
    "    path_graphs = OUTPUT_DIR / f\"selected_graphs_k{k}.json\"\n",
    "    path_scenarios = OUTPUT_DIR / f\"selected_scenarios_k{k}.json\"\n",
    "    if path_graphs.exists():\n",
    "        data = json.loads(path_graphs.read_text())\n",
    "        data['_path'] = path_graphs\n",
    "        data['_format'] = 'selected_graphs'\n",
    "        return data\n",
    "    if path_scenarios.exists():\n",
    "        data = json.loads(path_scenarios.read_text())\n",
    "        data['_path'] = path_scenarios\n",
    "        data['_format'] = 'selected_scenarios'\n",
    "        return data\n",
    "    raise FileNotFoundError(f\"No selection file found for k={k} in {OUTPUT_DIR}\")\n",
    "\n",
    "\n",
    "selections = {k: load_selection(k) for k in K_VALUES}\n",
    "for k, data in selections.items():\n",
    "    print(f\"k={k} | format={data['_format']} | path={data['_path'].name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8187a5a9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-26T18:26:49.171284Z",
     "iopub.status.busy": "2026-01-26T18:26:49.171186Z",
     "iopub.status.idle": "2026-01-26T18:26:49.243488Z",
     "shell.execute_reply": "2026-01-26T18:26:49.243206Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 49125 graphs using pattern 'graph_*.pickle'\n"
     ]
    }
   ],
   "source": [
    "# Build graph file index (for validation)\n",
    "pattern = None\n",
    "for data in selections.values():\n",
    "    if 'pattern' in data:\n",
    "        pattern = data['pattern']\n",
    "        break\n",
    "if pattern is None:\n",
    "    pattern = 'graph_*.pickle'\n",
    "\n",
    "all_graph_files = sorted(glob.glob(str(GRAPHS_DIR / pattern)))\n",
    "all_graph_basenames = [os.path.basename(p) for p in all_graph_files]\n",
    "name_to_index = {name: idx for idx, name in enumerate(all_graph_basenames)}\n",
    "\n",
    "print(f\"Found {len(all_graph_files)} graphs using pattern '{pattern}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dc60e60f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-26T18:26:49.244808Z",
     "iopub.status.busy": "2026-01-26T18:26:49.244730Z",
     "iopub.status.idle": "2026-01-26T18:26:49.252481Z",
     "shell.execute_reply": "2026-01-26T18:26:49.252234Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ok] k=20: all selected files exist\n",
      "[ok] k=20: selected_indices match filenames\n",
      "[ok] k=20: cluster_sizes match label counts\n",
      "[ok] k=20: medoid labels consistent\n",
      "[ok] k=100: all selected files exist\n",
      "[ok] k=100: selected_indices match filenames\n",
      "[ok] k=100: cluster_sizes match label counts\n",
      "[ok] k=100: medoid labels consistent\n"
     ]
    }
   ],
   "source": [
    "# Parse selection outputs into a common shape and validate file selection\n",
    "\n",
    "def parse_selection(data, k):\n",
    "    selected_files = []\n",
    "    selected_indices = None\n",
    "    cluster_labels = None\n",
    "    cluster_sizes = None\n",
    "\n",
    "    if data['_format'] == 'selected_graphs':\n",
    "        selected_files = data['selected_pickle_files']\n",
    "        selected_indices = data.get('selected_indices')\n",
    "        cluster_labels = np.array(data.get('cluster_labels', []), dtype=int)\n",
    "        cluster_sizes = data.get('cluster_sizes')\n",
    "    else:\n",
    "        selected_files = [s['filename'] for s in data['selected_scenarios']]\n",
    "        cluster_sizes = [s['cluster_size'] for s in data['selected_scenarios']]\n",
    "        labels_path = OUTPUT_DIR / f\"cluster_assignments_k{k}.npy\"\n",
    "        if labels_path.exists():\n",
    "            cluster_labels = np.load(labels_path)\n",
    "\n",
    "    # Validate selected files exist\n",
    "    missing = [f for f in selected_files if f not in name_to_index]\n",
    "    if missing:\n",
    "        print(f\"[warn] k={k}: missing files: {missing[:5]}{'...' if len(missing) > 5 else ''}\")\n",
    "    else:\n",
    "        print(f\"[ok] k={k}: all selected files exist\")\n",
    "\n",
    "    # Validate index mapping if provided\n",
    "    if selected_indices is not None:\n",
    "        mismatches = []\n",
    "        for idx, fname in zip(selected_indices, selected_files):\n",
    "            if idx >= len(all_graph_basenames) or all_graph_basenames[idx] != fname:\n",
    "                mismatches.append((idx, fname))\n",
    "        if mismatches:\n",
    "            print(f\"[warn] k={k}: index->filename mismatches: {mismatches[:3]}\")\n",
    "        else:\n",
    "            print(f\"[ok] k={k}: selected_indices match filenames\")\n",
    "\n",
    "    # Validate cluster sizes if labels are present\n",
    "    label_counts = None\n",
    "    if cluster_labels is not None and cluster_labels.size > 0:\n",
    "        label_counts = np.bincount(cluster_labels, minlength=k)\n",
    "        if cluster_sizes is not None and len(cluster_sizes) == k:\n",
    "            if not np.all(label_counts == np.array(cluster_sizes)):\n",
    "                print(f\"[warn] k={k}: cluster_sizes do not match label counts\")\n",
    "            else:\n",
    "                print(f\"[ok] k={k}: cluster_sizes match label counts\")\n",
    "        if selected_indices is not None:\n",
    "            # Medoid label should match its cluster id (order of medoids)\n",
    "            bad_medoids = []\n",
    "            for cluster_id, medoid_idx in enumerate(selected_indices):\n",
    "                if medoid_idx < len(cluster_labels) and cluster_labels[medoid_idx] != cluster_id:\n",
    "                    bad_medoids.append((cluster_id, medoid_idx, cluster_labels[medoid_idx]))\n",
    "            if bad_medoids:\n",
    "                print(f\"[warn] k={k}: medoid label mismatches: {bad_medoids[:3]}\")\n",
    "            else:\n",
    "                print(f\"[ok] k={k}: medoid labels consistent\")\n",
    "\n",
    "    return {\n",
    "        'selected_files': selected_files,\n",
    "        'selected_indices': selected_indices,\n",
    "        'cluster_labels': cluster_labels,\n",
    "        'cluster_sizes': cluster_sizes,\n",
    "        'label_counts': label_counts,\n",
    "    }\n",
    "\n",
    "\n",
    "parsed = {k: parse_selection(data, k) for k, data in selections.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a94635a4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-26T18:26:49.253669Z",
     "iopub.status.busy": "2026-01-26T18:26:49.253572Z",
     "iopub.status.idle": "2026-01-26T18:27:02.232616Z",
     "shell.execute_reply": "2026-01-26T18:27:02.232322Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded cached WL features: /Users/ignaciagothe/Desktop/proyecto graph kernels/graph_scenarios/src/kernels/outputs/wl_features_h3_d262144.npz | shape=(49125, 262144) nnz=19817686\n",
      "Xn ready: (49125, 262144) nnz 19817686\n"
     ]
    }
   ],
   "source": [
    "# --- WL features for full silhouette computation ---\n",
    "import sys\n",
    "from scipy import sparse\n",
    "from tqdm import tqdm\n",
    "\n",
    "kernels_dir = ROOT / 'src' / 'kernels'\n",
    "if str(kernels_dir) not in sys.path:\n",
    "    sys.path.append(str(kernels_dir))\n",
    "\n",
    "from kernel_baseline import DirectedWLKernelHasher, row_l2_norms, row_normalize, load_nx_dag\n",
    "\n",
    "# WL params from selection metadata (fallback defaults)\n",
    "meta = selections[K_VALUES[0]]\n",
    "wl_iterations = meta.get('wl_iterations', 3)\n",
    "hash_dim = meta.get('hash_dim', 2**18)\n",
    "node_label_attr = meta.get('node_label_attr')\n",
    "\n",
    "X_cache = OUTPUT_DIR / f\"wl_features_h{wl_iterations}_d{hash_dim}.npz\"\n",
    "if X_cache.exists():\n",
    "    X = sparse.load_npz(X_cache)\n",
    "    print(f\"Loaded cached WL features: {X_cache} | shape={X.shape} nnz={X.nnz}\")\n",
    "else:\n",
    "    hasher = DirectedWLKernelHasher(h=wl_iterations, hash_dim=hash_dim, node_label_attr=node_label_attr)\n",
    "    rows, cols, data = [], [], []\n",
    "    for i, path in enumerate(tqdm(all_graph_files, desc='Computing WL features')):\n",
    "        G = load_nx_dag(path)\n",
    "        fd = hasher._wl_features_one_graph(G)\n",
    "        for c, v in fd.items():\n",
    "            rows.append(i)\n",
    "            cols.append(c)\n",
    "            data.append(v)\n",
    "\n",
    "    X = sparse.csr_matrix((data, (rows, cols)), shape=(len(all_graph_files), hash_dim), dtype=np.float64)\n",
    "    sparse.save_npz(X_cache, X)\n",
    "    print(f\"Saved WL features: {X_cache} | shape={X.shape} nnz={X.nnz}\")\n",
    "\n",
    "norms = row_l2_norms(X)\n",
    "Xn = row_normalize(X, norms)\n",
    "print('Xn ready:', Xn.shape, 'nnz', Xn.nnz)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a4b53b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-26T18:27:02.234070Z",
     "iopub.status.busy": "2026-01-26T18:27:02.233979Z",
     "iopub.status.idle": "2026-01-26T18:27:02.341360Z",
     "shell.execute_reply": "2026-01-26T18:27:02.341075Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Silhouette (full):  37%|███▋      | 182/492 [01:22<02:23,  2.17it/s]"
     ]
    }
   ],
   "source": [
    "# --- Full silhouette scores (no sampling) ---\n",
    "from scipy import sparse\n",
    "\n",
    "\n",
    "def full_silhouette_samples_from_Xn(Xn, labels, chunk_size=100):\n",
    "    labels = np.asarray(labels, dtype=int)\n",
    "    n = Xn.shape[0]\n",
    "    k = int(labels.max()) + 1\n",
    "    cluster_sizes = np.bincount(labels, minlength=k).astype(np.int64)\n",
    "    if np.any(cluster_sizes == 0):\n",
    "        raise ValueError('Empty cluster detected in labels')\n",
    "\n",
    "    membership = sparse.csr_matrix(\n",
    "        (np.ones(n, dtype=np.float64), (np.arange(n), labels)),\n",
    "        shape=(n, k)\n",
    "    )\n",
    "\n",
    "    sil_samples = np.empty(n, dtype=np.float32)\n",
    "\n",
    "    for start in tqdm(range(0, n, chunk_size), desc='Silhouette (full)'):\n",
    "        end = min(start + chunk_size, n)\n",
    "        Xc = Xn[start:end]\n",
    "\n",
    "        # Similarity to all points\n",
    "        S = (Xc @ Xn.T).toarray()\n",
    "        np.clip(S, -1.0, 1.0, out=S)\n",
    "\n",
    "        # Convert similarity to Euclidean distance on unit sphere: sqrt(2 - 2*cos)\n",
    "        S *= -2.0\n",
    "        S += 2.0\n",
    "        np.maximum(S, 0.0, out=S)\n",
    "        np.sqrt(S, out=S)  # now distances\n",
    "\n",
    "        sums = S @ membership  # (m, k) sum distances to each cluster\n",
    "        labels_chunk = labels[start:end]\n",
    "        sizes = cluster_sizes[labels_chunk]\n",
    "\n",
    "        sums_own = sums[np.arange(end - start), labels_chunk]\n",
    "        a = np.zeros(end - start, dtype=np.float64)\n",
    "        mask = sizes > 1\n",
    "        a[mask] = sums_own[mask] / (sizes[mask] - 1)\n",
    "\n",
    "        mean_other = sums / cluster_sizes\n",
    "        mean_other[np.arange(end - start), labels_chunk] = np.inf\n",
    "        b = np.min(mean_other, axis=1)\n",
    "\n",
    "        s = (b - a) / np.maximum(a, b)\n",
    "        s[~np.isfinite(s)] = 0.0\n",
    "        sil_samples[start:end] = s.astype(np.float32)\n",
    "\n",
    "    return sil_samples\n",
    "\n",
    "\n",
    "full_silhouette = {}\n",
    "for k in K_VALUES:\n",
    "    labels = parsed[k]['cluster_labels']\n",
    "    if labels is None or len(labels) == 0:\n",
    "        print(f'k={k}: no cluster labels available for silhouette')\n",
    "        continue\n",
    "    if len(labels) != len(all_graph_files):\n",
    "        print(f'k={k}: label length {len(labels)} != n_graphs {len(all_graph_files)}, skipping')\n",
    "        continue\n",
    "\n",
    "    sil = full_silhouette_samples_from_Xn(Xn, labels, chunk_size=100)\n",
    "    full_silhouette[k] = sil\n",
    "    score = float(np.mean(sil))\n",
    "    print(f'k={k} full silhouette: {score:.4f}')\n",
    "\n",
    "    np.save(OUTPUT_DIR / f\"silhouette_samples_k{k}.npy\", sil)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01330f10",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-26T18:27:02.342617Z",
     "iopub.status.busy": "2026-01-26T18:27:02.342534Z",
     "iopub.status.idle": "2026-01-26T18:27:02.658931Z",
     "shell.execute_reply": "2026-01-26T18:27:02.658164Z"
    }
   },
   "outputs": [],
   "source": [
    "# Cluster size distributions\n",
    "for k in K_VALUES:\n",
    "    label_counts = parsed[k]['label_counts']\n",
    "    if label_counts is None:\n",
    "        print(f\"k={k}: no cluster labels available for size plots\")\n",
    "        continue\n",
    "\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(10, 4))\n",
    "    ax.bar(range(k), np.sort(label_counts)[::-1], color='steelblue', edgecolor='black')\n",
    "    ax.set_title(f'Cluster Sizes (k={k}) | min={label_counts.min()}, mean={label_counts.mean():.1f}, max={label_counts.max()}')\n",
    "    ax.set_xlabel('Cluster (sorted by size)')\n",
    "    ax.set_ylabel('Cluster size')\n",
    "    ax.axhline(label_counts.mean(), color='red', linestyle='--', label='Mean')\n",
    "    ax.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a81eee33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Silhouette & cluster visualizations (full) ---\n",
    "for k in K_VALUES:\n",
    "    if k not in full_silhouette:\n",
    "        continue\n",
    "\n",
    "    sil = full_silhouette[k]\n",
    "    labels = np.asarray(parsed[k]['cluster_labels'], dtype=int)\n",
    "    cluster_sizes = np.bincount(labels, minlength=k)\n",
    "    cluster_means = np.array([\n",
    "        sil[labels == c].mean() if cluster_sizes[c] > 0 else np.nan\n",
    "        for c in range(k)\n",
    "    ])\n",
    "\n",
    "    # 1) Silhouette distribution\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(8, 4))\n",
    "    ax.hist(sil, bins=50, color='teal', edgecolor='black', alpha=0.7)\n",
    "    ax.axvline(sil.mean(), color='red', linestyle='--', label=f\"Mean: {sil.mean():.4f}\")\n",
    "    ax.set_title(f\"Silhouette Distribution (k={k})\")\n",
    "    ax.set_xlabel('Silhouette coefficient')\n",
    "    ax.set_ylabel('Frequency')\n",
    "    ax.legend()\n",
    "    ax.grid(alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # 2) Mean silhouette per cluster (sorted)\n",
    "    order = np.argsort(cluster_means)[::-1]\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(10, 4))\n",
    "    ax.bar(range(k), cluster_means[order], color='steelblue', edgecolor='black')\n",
    "    ax.set_title(f\"Mean Silhouette by Cluster (k={k})\")\n",
    "    ax.set_xlabel('Cluster (sorted by mean silhouette)')\n",
    "    ax.set_ylabel('Mean silhouette')\n",
    "    ax.grid(alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # 3) Cluster size vs mean silhouette\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(6, 4))\n",
    "    ax.scatter(cluster_sizes, cluster_means, alpha=0.6)\n",
    "    ax.set_title(f\"Cluster Size vs Mean Silhouette (k={k})\")\n",
    "    ax.set_xlabel('Cluster size')\n",
    "    ax.set_ylabel('Mean silhouette')\n",
    "    ax.grid(alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # 4) Cumulative coverage\n",
    "    weights = cluster_sizes / cluster_sizes.sum()\n",
    "    cumulative = np.cumsum(np.sort(weights)[::-1])\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(6, 4))\n",
    "    ax.plot(range(1, k + 1), cumulative * 100, linewidth=2)\n",
    "    ax.axhline(50, color='red', linestyle='--', alpha=0.5, label='50%')\n",
    "    ax.axhline(80, color='orange', linestyle='--', alpha=0.5, label='80%')\n",
    "    ax.set_title(f\"Cumulative Coverage (k={k})\")\n",
    "    ax.set_xlabel('Number of clusters')\n",
    "    ax.set_ylabel('Coverage (%)')\n",
    "    ax.legend()\n",
    "    ax.grid(alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a4f051a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-26T18:27:02.662028Z",
     "iopub.status.busy": "2026-01-26T18:27:02.661861Z",
     "iopub.status.idle": "2026-01-26T18:27:05.635090Z",
     "shell.execute_reply": "2026-01-26T18:27:05.634760Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_graph_by_name(filename):\n",
    "    with open(GRAPHS_DIR / filename, 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "\n",
    "def graph_stats(G):\n",
    "    return {\n",
    "        'nodes': G.number_of_nodes(),\n",
    "        'edges': G.number_of_edges(),\n",
    "        'density': nx.density(G) if G.number_of_nodes() > 1 else 0.0,\n",
    "    }\n",
    "\n",
    "\n",
    "def grid_positions_20(G):\n",
    "    nodes = list(G.nodes())\n",
    "    if not nodes:\n",
    "        return None\n",
    "    if not all(isinstance(n, (int, np.integer)) for n in nodes):\n",
    "        return None\n",
    "    min_n = int(min(nodes))\n",
    "    max_n = int(max(nodes))\n",
    "    if 0 <= min_n and max_n <= 399:\n",
    "        offset = 0\n",
    "    elif 1 <= min_n and max_n <= 400:\n",
    "        offset = 1\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "    pos = {}\n",
    "    for node in nodes:\n",
    "        idx = int(node) - offset\n",
    "        row = idx // 20\n",
    "        col = idx % 20\n",
    "        pos[node] = (col, 19 - row)\n",
    "    return pos\n",
    "\n",
    "\n",
    "def draw_fire_spread_graph(G, ax, title=''):\n",
    "    pos = grid_positions_20(G)\n",
    "    if pos is None:\n",
    "        pos = nx.spring_layout(G, seed=42)\n",
    "\n",
    "    if G.is_directed():\n",
    "        ignition_nodes = [n for n in G.nodes() if G.in_degree(n) == 0]\n",
    "    else:\n",
    "        ignition_nodes = []\n",
    "\n",
    "    node_colors = ['red' if n in ignition_nodes else 'orange' for n in G.nodes()]\n",
    "\n",
    "    nx.draw_networkx_nodes(G, pos, ax=ax, node_size=30, node_color=node_colors, alpha=0.8)\n",
    "    nx.draw_networkx_edges(G, pos, ax=ax, edge_color='gray', alpha=0.5,\n",
    "                           arrows=G.is_directed(), arrowsize=5, width=0.5)\n",
    "\n",
    "    ax.set_title(title, fontsize=8)\n",
    "    ax.set_axis_off()\n",
    "\n",
    "\n",
    "def plot_selected_graphs(k):\n",
    "    selected_files = parsed[k]['selected_files']\n",
    "    if not selected_files:\n",
    "        print(f\"k={k}: no selected files to plot\")\n",
    "        return\n",
    "\n",
    "    graphs = []\n",
    "    for fname in selected_files:\n",
    "        G = load_graph_by_name(fname)\n",
    "        stats = graph_stats(G)\n",
    "        graphs.append((fname, G, stats))\n",
    "\n",
    "    cols = 5\n",
    "    rows = int(np.ceil(len(graphs) / cols))\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(cols * 3.2, rows * 3.0))\n",
    "    axes = np.array(axes).reshape(-1)\n",
    "\n",
    "    for i, (fname, G, stats) in enumerate(graphs):\n",
    "        title = f\"{fname}\\n{stats['nodes']}n, {stats['edges']}e\"\n",
    "        draw_fire_spread_graph(G, axes[i], title)\n",
    "\n",
    "    for j in range(i + 1, len(axes)):\n",
    "        axes[j].axis('off')\n",
    "\n",
    "    plt.suptitle(f\"Selected Graphs (k={k})\", fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_selected_graphs(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb44a84c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-26T18:27:05.636971Z",
     "iopub.status.busy": "2026-01-26T18:27:05.636869Z",
     "iopub.status.idle": "2026-01-26T18:27:19.874024Z",
     "shell.execute_reply": "2026-01-26T18:27:19.873675Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_selected_graphs(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c062dc8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
